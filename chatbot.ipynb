{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWaUN9Py5u1D",
        "outputId": "d23850ef-4419-4c45-b544-5f0f72f5119f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=cfd3940cd603a9fb31a4badf83d93e93478e89cde324be70373cac8e3b0da1a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import sys\n",
        "import argparse\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDJJ1xKe5puP",
        "outputId": "5a26fe4f-d268-49af-9655-9de5a547964b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1. DATA PREPROCESSING & NORMALIZATION\n",
        "\n"
      ],
      "metadata": {
        "id": "zyeLC4cD58J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def normalize_urdu_text(text):\n",
        "    \"\"\"\n",
        "    Normalize Urdu text by:\n",
        "    1. Removing diacritics\n",
        "    2. Standardizing Alef forms\n",
        "    3. Standardizing Yeh forms\n",
        "    \"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove Arabic diacritical marks (Harakat)\n",
        "    # These include: Fatha, Damma, Kasra, Sukun, Shadda, Tanwin, etc.\n",
        "    diacritics = [\n",
        "        '\\u0610', '\\u0611', '\\u0612', '\\u0613', '\\u0614', '\\u0615', '\\u0616',\n",
        "        '\\u0617', '\\u0618', '\\u0619', '\\u061A', '\\u064B', '\\u064C', '\\u064D',\n",
        "        '\\u064E', '\\u064F', '\\u0650', '\\u0651', '\\u0652', '\\u0653', '\\u0654',\n",
        "        '\\u0655', '\\u0656', '\\u0657', '\\u0658', '\\u0659', '\\u065A', '\\u065B',\n",
        "        '\\u065C', '\\u065D', '\\u065E', '\\u065F', '\\u0670', '\\u06D6', '\\u06D7',\n",
        "        '\\u06D8', '\\u06D9', '\\u06DA', '\\u06DB', '\\u06DC', '\\u06DF', '\\u06E0',\n",
        "        '\\u06E1', '\\u06E2', '\\u06E3', '\\u06E4', '\\u06E7', '\\u06E8', '\\u06EA',\n",
        "        '\\u06EB', '\\u06EC', '\\u06ED'\n",
        "    ]\n",
        "\n",
        "    for diacritic in diacritics:\n",
        "        text = text.replace(diacritic, '')\n",
        "\n",
        "    # Standardize Alef forms to regular Alef (ÿß)\n",
        "    # ÿ£ (Alef with Hamza above), ÿ• (Alef with Hamza below), ÿ¢ (Alef with Madda)\n",
        "    text = re.sub('[ÿ£ÿ•Ÿ±]', 'ÿß', text)\n",
        "\n",
        "    # Standardize Yeh forms to Urdu Yeh (€å)\n",
        "    # Ÿâ (Arabic Yeh), ÿ¶ (Yeh with Hamza), Ÿä (Arabic Yeh)\n",
        "    text = re.sub('[ŸäŸâÿ¶]', '€å', text)\n",
        "\n",
        "    # Remove Zero-width non-joiner (ZWNJ) and Zero-width joiner (ZWJ)\n",
        "    text = text.replace('\\u200c', '').replace('\\u200d', '')\n",
        "\n",
        "    # Normalize fancy/smart quotation marks to ASCII\n",
        "    text = text.replace('\\u2018', \"'\")  # ' ‚Üí '\n",
        "    text = text.replace('\\u2019', \"'\")  # ' ‚Üí '\n",
        "    text = text.replace('\\u201C', '\"')  # \" ‚Üí \"\n",
        "    text = text.replace('\\u201D', '\"')  # \" ‚Üí \"\n",
        "    text = text.replace('\\u201A', \"'\")  # ‚Äö ‚Üí '\n",
        "    text = text.replace('\\u201E', '\"')  # ‚Äû ‚Üí \"\n",
        "\n",
        "    # Normalize special punctuation\n",
        "    text = text.replace('\\u2026', '...')  # ‚Ä¶ ‚Üí ...\n",
        "    text = text.replace('\\u2013', '-')    # ‚Äì ‚Üí -\n",
        "    text = text.replace('\\u2014', '-')    # ‚Äî ‚Üí -\n",
        "    text = text.replace('`', \"'\")         # ` ‚Üí '\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def process_tsv_file(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Process the TSV file and extract cleaned Urdu sentences\n",
        "    \"\"\"\n",
        "    cleaned_sentences = []\n",
        "\n",
        "    try:\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            # Skip header line\n",
        "            next(f)\n",
        "\n",
        "            for line_num, line in enumerate(f, start=2):\n",
        "                try:\n",
        "                    # Split by tab and extract sentence column (index 2)\n",
        "                    columns = line.strip().split('\\t')\n",
        "                    if len(columns) > 2:\n",
        "                        sentence = columns[2]\n",
        "                        if sentence:  # Only process non-empty sentences\n",
        "                            cleaned_sentence = normalize_urdu_text(sentence)\n",
        "                            if cleaned_sentence:  # Only add non-empty cleaned sentences\n",
        "                                cleaned_sentences.append(cleaned_sentence)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing line {line_num}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # Write cleaned sentences to output file\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            for sentence in cleaned_sentences:\n",
        "                f.write(sentence + '\\n')\n",
        "\n",
        "        print(f\"Successfully processed {len(cleaned_sentences)} sentences\")\n",
        "        print(f\"Cleaned text saved to: {output_file}\")\n",
        "\n",
        "        # Show some examples\n",
        "        print(\"\\nFirst 5 cleaned sentences:\")\n",
        "        for i, sentence in enumerate(cleaned_sentences[:5], 1):\n",
        "            print(f\"{i}. {sentence}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{input_file}' not found\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"final_main_dataset.tsv\"\n",
        "    output_file = \"cleaned_urdu_text.txt\"\n",
        "\n",
        "    process_tsv_file(input_file, output_file)"
      ],
      "metadata": {
        "id": "BdFnkwnP5764",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a55d40-1f0a-4a89-c2ce-fbc2141e62f5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 20044 sentences\n",
            "Cleaned text saved to: cleaned_urdu_text.txt\n",
            "\n",
            "First 5 cleaned sentences:\n",
            "1. ⁄©ÿ®⁄æ€å ⁄©ÿ®⁄æÿßÿ± €Å€å ÿÆ€åÿßŸÑ€å ŸæŸÑÿßŸà ÿ®ŸÜÿßÿ™ÿß €ÅŸà⁄∫\n",
            "2. ÿßŸàÿ± Ÿæ⁄æÿ± ŸÖŸÖ⁄©ŸÜ €Å€í ⁄©€Å Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ÿ®⁄æ€å €ÅŸà\n",
            "3. €å€Å ŸÅ€åÿµŸÑ€Å ÿ®⁄æ€å ⁄Øÿ≤ÿ¥ÿ™€Å ÿØŸà ÿ≥ÿßŸÑ ŸÖ€å⁄∫\n",
            "4. ÿßŸÜ ⁄©€í ÿ®ŸÑ€í ÿ®ÿßÿ≤Ÿà⁄∫ ⁄©€í ÿ≥ÿßŸÖŸÜ€í €ÅŸà ⁄Øÿß\n",
            "5. ÿ¢ÿ®€å ÿ¨ÿßŸÜŸàÿ± ŸÖ€å⁄∫ ÿ®ÿ∑ÿÆ ÿ®⁄ØŸÑÿß ÿßŸàÿ± ÿØŸàÿ≥ÿ±ÿß ÿ¢ÿ®€å Ÿæÿ±ŸÜÿØ€Å ÿ¥ÿßŸÖŸÑ €ÅŸàŸÜÿß\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. BPE TOKENIZER\n"
      ],
      "metadata": {
        "id": "xofpSDel6IuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BPETokenizer:\n",
        "    \"\"\"Byte Pair Encoding tokenizer with RTL support for Urdu\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int = 5000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = []\n",
        "        self.merges = []\n",
        "        self.token_to_id = {}\n",
        "        self.id_to_token = {}\n",
        "\n",
        "    def get_stats(self, words: Dict[str, int]) -> Counter:\n",
        "        \"\"\"Count frequency of adjacent pairs in the corpus\"\"\"\n",
        "        pairs = Counter()\n",
        "        for word, freq in words.items():\n",
        "            symbols = word.split()\n",
        "            for i in range(len(symbols) - 1):\n",
        "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "        return pairs\n",
        "\n",
        "    def merge_pair(self, pair: Tuple[str, str], words: Dict[str, int]) -> Dict[str, int]:\n",
        "        \"\"\"Merge all occurrences of the most frequent pair\"\"\"\n",
        "        new_words = {}\n",
        "        bigram = ' '.join(pair)\n",
        "        replacement = ''.join(pair)\n",
        "        for word in words:\n",
        "            new_word = word.replace(bigram, replacement)\n",
        "            new_words[new_word] = words[word]\n",
        "        return new_words\n",
        "\n",
        "    def train(self, corpus: List[str], verbose: bool = True):\n",
        "        \"\"\"Train BPE on corpus\"\"\"\n",
        "        word_freqs = Counter()\n",
        "        for text in corpus:\n",
        "            word_freqs.update(text.split())\n",
        "\n",
        "        # RTL-aware: </w> at beginning\n",
        "        words = {}\n",
        "        for word, freq in word_freqs.items():\n",
        "            words['</w> ' + ' '.join(list(word))] = freq\n",
        "\n",
        "        vocab = set()\n",
        "        for word in words.keys():\n",
        "            vocab.update(word.split())\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Initial vocab: {len(vocab)} characters\")\n",
        "\n",
        "        num_merges = self.vocab_size - len(vocab)\n",
        "        for i in range(num_merges):\n",
        "            pairs = self.get_stats(words)\n",
        "            if not pairs:\n",
        "                break\n",
        "            best_pair = max(pairs, key=pairs.get)\n",
        "            words = self.merge_pair(best_pair, words)\n",
        "            self.merges.append(best_pair)\n",
        "\n",
        "        final_vocab = set()\n",
        "        for word in words.keys():\n",
        "            final_vocab.update(word.split())\n",
        "\n",
        "        self.vocab = sorted(list(final_vocab))\n",
        "        self.token_to_id = {token: idx for idx, token in enumerate(self.vocab)}\n",
        "        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Final vocab: {len(self.vocab)} tokens\")\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenize text using BPE\"\"\"\n",
        "        words = text.split()\n",
        "        tokens = []\n",
        "        for word in words:\n",
        "            word_tokens = ['</w>'] + list(word)\n",
        "            for pair in self.merges:\n",
        "                i = 0\n",
        "                while i < len(word_tokens) - 1:\n",
        "                    if (word_tokens[i], word_tokens[i + 1]) == pair:\n",
        "                        word_tokens = word_tokens[:i] + [''.join(pair)] + word_tokens[i + 2:]\n",
        "                    else:\n",
        "                        i += 1\n",
        "            tokens.extend(word_tokens)\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        \"\"\"Encode text to token IDs\"\"\"\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.token_to_id.get(token, self.token_to_id.get('<UNK>', 0)) for token in tokens]\n",
        "\n",
        "    def decode(self, token_ids: List[int]) -> str:\n",
        "        \"\"\"Decode token IDs to text\"\"\"\n",
        "        tokens = [self.id_to_token.get(idx, '<UNK>') for idx in token_ids]\n",
        "        text = ''.join(tokens).replace('</w> ', ' ').replace('</w>', ' ')\n",
        "        return text.strip()\n"
      ],
      "metadata": {
        "id": "vEbR_Z226JaQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. DATASET PREPARATION\n"
      ],
      "metadata": {
        "id": "ZtwjIbUx6Ufd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class UrduChatbotDataset(Dataset):\n",
        "    \"\"\"Dataset for Urdu chatbot with teacher forcing\"\"\"\n",
        "\n",
        "    def __init__(self, sentence_groups, tokenizer, max_len=50):\n",
        "        self.sentence_groups = sentence_groups\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Special tokens\n",
        "        self.pad_token = '<PAD>'\n",
        "        self.start_token = '<START>'\n",
        "        self.end_token = '<END>'\n",
        "        self.unk_token = '<UNK>'\n",
        "\n",
        "        # Add special tokens to vocabulary\n",
        "        for token in [self.pad_token, self.start_token, self.end_token, self.unk_token]:\n",
        "            if token not in self.tokenizer.token_to_id:\n",
        "                idx = len(self.tokenizer.token_to_id)\n",
        "                self.tokenizer.token_to_id[token] = idx\n",
        "                self.tokenizer.id_to_token[idx] = token\n",
        "\n",
        "        self.pad_idx = self.tokenizer.token_to_id[self.pad_token]\n",
        "        self.start_idx = self.tokenizer.token_to_id[self.start_token]\n",
        "        self.end_idx = self.tokenizer.token_to_id[self.end_token]\n",
        "        self.unk_idx = self.tokenizer.token_to_id[self.unk_token]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentence_groups)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Split based on word count (2/5th for input, 3/5th for target)\n",
        "        group = self.sentence_groups[idx]\n",
        "        words = group.split()\n",
        "\n",
        "        # Calculate split point based on word count\n",
        "        total_words = len(words)\n",
        "        split_point = max(1, total_words * 2 // 5)  # At least 1 word for input\n",
        "\n",
        "        input_text = ' '.join(words[:split_point])\n",
        "        target_text = ' '.join(words[split_point:])\n",
        "\n",
        "        # Tokenize input and target\n",
        "        input_tokens = self.tokenizer.tokenize(input_text)\n",
        "        target_tokens = self.tokenizer.tokenize(target_text)\n",
        "\n",
        "        # Convert to IDs\n",
        "        input_ids = [self.tokenizer.token_to_id.get(t, self.unk_idx) for t in input_tokens]\n",
        "        target_ids = [self.tokenizer.token_to_id.get(t, self.unk_idx) for t in target_tokens]\n",
        "\n",
        "        # Truncate and pad\n",
        "        input_ids = input_ids[:self.max_len] + [self.pad_idx] * (self.max_len - len(input_ids))\n",
        "        target_ids = target_ids[:self.max_len] + [self.pad_idx] * (self.max_len - len(target_ids))\n",
        "\n",
        "        # Teacher forcing: decoder input is [START] + target[:-1], decoder target is target\n",
        "        decoder_input_ids = [self.start_idx] + target_ids[:-1]\n",
        "        decoder_target_ids = target_ids\n",
        "\n",
        "        return {\n",
        "            'encoder_input': torch.tensor(input_ids, dtype=torch.long),\n",
        "            'decoder_input': torch.tensor(decoder_input_ids, dtype=torch.long),\n",
        "            'decoder_target': torch.tensor(decoder_target_ids, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "N8fQMRx46VHp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. TRANSFORMER ARCHITECTURE\n"
      ],
      "metadata": {
        "id": "By_7TkgG6dby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding using sine and cosine\"\"\"\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Attention mechanism\"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        return output, attention_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        Q = self.split_heads(self.W_q(query))\n",
        "        K = self.split_heads(self.W_k(key))\n",
        "        V = self.split_heads(self.W_v(value))\n",
        "        attn_output, _ = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
        "        output = self.W_o(attn_output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"Single Transformer Encoder Layer\"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.self_attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"Single Transformer Decoder Layer\"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        self_attn_output = self.self_attention(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout1(self_attn_output))\n",
        "        cross_attn_output = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout2(cross_attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout3(ff_output))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"Complete Transformer Encoder-Decoder Model\"\"\"\n",
        "    def __init__(self, vocab_size, d_model=256, num_heads=2, d_ff=1024,\n",
        "                 num_encoder_layers=2, num_decoder_layers=2, max_len=512,\n",
        "                 dropout=0.1, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "        self.encoder_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.encoder_pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
        "        self.decoder_pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "        self._init_parameters()\n",
        "\n",
        "    def _init_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        batch_size, tgt_len = tgt.size()\n",
        "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
        "        tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
        "        return tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "\n",
        "        # Encoder\n",
        "        x = self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
        "        x = self.encoder_pos_encoding(x)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, src_mask)\n",
        "        encoder_output = x\n",
        "\n",
        "        # Decoder\n",
        "        x = self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        x = self.decoder_pos_encoding(x)\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.output_projection(x)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "57kD9MqX6d-B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. TRAINING FUNCTIONS\n"
      ],
      "metadata": {
        "id": "yKm08yft6m13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch with teacher forcing\"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    pbar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for batch in pbar:\n",
        "        encoder_input = batch['encoder_input'].to(device)\n",
        "        decoder_input = batch['decoder_input'].to(device)\n",
        "        decoder_target = batch['decoder_target'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(encoder_input, decoder_input)\n",
        "        output = output.reshape(-1, output.size(-1))\n",
        "        target = decoder_target.reshape(-1)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    return epoch_loss / len(train_loader)\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(val_loader, desc=\"Validation\")\n",
        "        for batch in pbar:\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            decoder_input = batch['decoder_input'].to(device)\n",
        "            decoder_target = batch['decoder_target'].to(device)\n",
        "\n",
        "            output = model(encoder_input, decoder_input)\n",
        "            output = output.reshape(-1, output.size(-1))\n",
        "            target = decoder_target.reshape(-1)\n",
        "            loss = criterion(output, target)\n",
        "            epoch_loss += loss.item()\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    return epoch_loss / len(val_loader)"
      ],
      "metadata": {
        "id": "G8FRmgKQ6nXQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. EVALUATION METRICS\n"
      ],
      "metadata": {
        "id": "dMAQS0Wp6tKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu_score(predicted, reference):\n",
        "    \"\"\"Calculate BLEU score\"\"\"\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    reference_tokens = reference.split()\n",
        "    predicted_tokens = predicted.split()\n",
        "\n",
        "    if len(reference_tokens) == 0 or len(predicted_tokens) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return sentence_bleu([reference_tokens], predicted_tokens, smoothing_function=smoothing)\n",
        "\n",
        "def calculate_rouge_l(predicted, reference):\n",
        "    \"\"\"Calculate ROUGE-L score\"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference, predicted)\n",
        "    return scores['rougeL'].fmeasure\n",
        "\n",
        "def calculate_chrf(predicted, reference):\n",
        "    \"\"\"Calculate chrF score (character-level F-score)\"\"\"\n",
        "    def get_char_ngrams(text, n):\n",
        "        return [text[i:i+n] for i in range(len(text)-n+1)]\n",
        "\n",
        "    def f_score(precision, recall):\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "        return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    total_f = 0.0\n",
        "    for n in range(1, 7):  # 1-6 character n-grams\n",
        "        pred_ngrams = get_char_ngrams(predicted, n)\n",
        "        ref_ngrams = get_char_ngrams(reference, n)\n",
        "\n",
        "        if len(pred_ngrams) == 0 and len(ref_ngrams) == 0:\n",
        "            continue\n",
        "\n",
        "        precision = len(set(pred_ngrams) & set(ref_ngrams)) / max(len(pred_ngrams), 1)\n",
        "        recall = len(set(pred_ngrams) & set(ref_ngrams)) / max(len(ref_ngrams), 1)\n",
        "\n",
        "        total_f += f_score(precision, recall)\n",
        "\n",
        "    return total_f / 6\n",
        "\n",
        "def calculate_perplexity(model, test_loader, device):\n",
        "    \"\"\"Calculate perplexity on test set\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            decoder_input = batch['decoder_input'].to(device)\n",
        "            decoder_target = batch['decoder_target'].to(device)\n",
        "\n",
        "            output = model(encoder_input, decoder_input)\n",
        "            output = output.reshape(-1, output.size(-1))\n",
        "            target = decoder_target.reshape(-1)\n",
        "\n",
        "            loss = F.cross_entropy(output, target, reduction='sum')\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += (target != 0).sum().item()  # Exclude padding tokens\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    return perplexity\n"
      ],
      "metadata": {
        "id": "lQhqFKM16rCY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. TEXT GENERATION\n"
      ],
      "metadata": {
        "id": "sJKStk4Y6zMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, dataset, input_text, max_length=30, temperature=0.8, device='cpu'):\n",
        "    \"\"\"Generate text continuation using beam search\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tokens = tokenizer.tokenize(input_text)\n",
        "        token_ids = [tokenizer.token_to_id.get(t, dataset.unk_idx) for t in tokens]\n",
        "        encoder_ids = token_ids[:50] + [dataset.pad_idx] * (50 - len(token_ids))\n",
        "        encoder_input = torch.tensor([encoder_ids], dtype=torch.long).to(device)\n",
        "        decoder_input = torch.tensor([[dataset.start_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "        generated_tokens = []\n",
        "        for _ in range(max_length):\n",
        "            output = model(encoder_input, decoder_input)\n",
        "            next_token_logits = output[0, -1, :] / temperature\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            if next_token.item() == dataset.end_idx:\n",
        "                break\n",
        "\n",
        "            generated_tokens.append(next_token.item())\n",
        "            decoder_input = torch.cat([decoder_input, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "        output_tokens = []\n",
        "        for idx in generated_tokens:\n",
        "            if idx not in [dataset.pad_idx, dataset.start_idx, dataset.unk_idx]:\n",
        "                token = tokenizer.id_to_token.get(idx, '<UNK>')\n",
        "                output_tokens.append(token)\n",
        "\n",
        "        text = ''.join(output_tokens).replace('</w> ', ' ').replace('</w>', ' ')\n",
        "        return text.strip()\n"
      ],
      "metadata": {
        "id": "9u7gZqi96zj3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. MAIN FUNCTIONS\n"
      ],
      "metadata": {
        "id": "6oVjWMLM63aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    \"\"\"Load sentence groups from file\"\"\"\n",
        "    print(\"Loading sentence groups...\")\n",
        "    with open('sentence_groups.txt', 'r', encoding='utf-8') as f:\n",
        "        sentence_groups = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    print(f\"Loaded {len(sentence_groups)} sentence groups\")\n",
        "    return sentence_groups\n",
        "\n",
        "def train_model(resume=False):\n",
        "    \"\"\"Train the Urdu chatbot model\"\"\"\n",
        "    if resume:\n",
        "        print(\"üîÑ Resuming Urdu Chatbot Training\")\n",
        "        print(\"=\" * 60)\n",
        "    else:\n",
        "        print(\"üöÄ Starting Urdu Chatbot Training\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    # Configuration following project specs\n",
        "    config = {\n",
        "        'vocab_size': 800,\n",
        "        'd_model': 512,  # Embedding dimensions\n",
        "        'num_heads': 2,  # Number of attention heads\n",
        "        'd_ff': 1024,    # Feed-forward dimensions\n",
        "        'num_encoder_layers': 2,\n",
        "        'num_decoder_layers': 2,\n",
        "        'max_len': 50,\n",
        "        'dropout': 0.1,\n",
        "        'batch_size': 32,\n",
        "        'num_epochs': 10,\n",
        "        'learning_rate': 1e-4,  # Adam optimizer learning rate\n",
        "    }\n",
        "\n",
        "    # Load data\n",
        "    sentence_groups = load_data()\n",
        "\n",
        "    # Normalize text\n",
        "    print(\"Normalizing text...\")\n",
        "    normalized_groups = [normalize_urdu_text(group) for group in sentence_groups]\n",
        "\n",
        "    # Train tokenizer\n",
        "    print(\"Training BPE tokenizer...\")\n",
        "    tokenizer = BPETokenizer(vocab_size=config['vocab_size'])\n",
        "    tokenizer.train(normalized_groups, verbose=True)\n",
        "\n",
        "    # Split dataset: 80% train, 10% validation, 10% test\n",
        "    print(\"Splitting dataset...\")\n",
        "    random.shuffle(normalized_groups)\n",
        "    train_size = int(0.8 * len(normalized_groups))\n",
        "    val_size = int(0.1 * len(normalized_groups))\n",
        "\n",
        "    train_groups = normalized_groups[:train_size]\n",
        "    val_groups = normalized_groups[train_size:train_size + val_size]\n",
        "    test_groups = normalized_groups[train_size + val_size:]\n",
        "\n",
        "    print(f\"Train: {len(train_groups)}, Val: {len(val_groups)}, Test: {len(test_groups)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"Creating datasets...\")\n",
        "    train_dataset = UrduChatbotDataset(train_groups, tokenizer, config['max_len'])\n",
        "    val_dataset = UrduChatbotDataset(val_groups, tokenizer, config['max_len'])\n",
        "    test_dataset = UrduChatbotDataset(test_groups, tokenizer, config['max_len'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "    vocab_size = len(tokenizer.token_to_id)\n",
        "\n",
        "    # Create model\n",
        "    print(\"Creating Transformer model...\")\n",
        "    model = Transformer(\n",
        "        vocab_size=vocab_size,\n",
        "        d_model=config['d_model'],\n",
        "        num_heads=config['num_heads'],\n",
        "        d_ff=config['d_ff'],\n",
        "        num_encoder_layers=config['num_encoder_layers'],\n",
        "        num_decoder_layers=config['num_decoder_layers'],\n",
        "        max_len=config['max_len'],\n",
        "        dropout=config['dropout'],\n",
        "        pad_idx=train_dataset.pad_idx\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Training setup\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.pad_idx)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.98))\n",
        "\n",
        "    # Load checkpoint if resuming\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    if resume:\n",
        "        try:\n",
        "            checkpoint = torch.load('best_model_allinone.pt', map_location=device)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        except FileNotFoundError:\n",
        "            print(\"‚ùå No checkpoint found! Starting fresh training.\")\n",
        "            resume = False\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading checkpoint: {e}\")\n",
        "            print(\"Starting fresh training.\")\n",
        "            resume = False\n",
        "\n",
        "    print(\"\\nüéØ Starting Training Loop\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']}\")\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save best model based on validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'tokenizer': tokenizer,\n",
        "                'config': config,\n",
        "                'vocab_size': vocab_size,\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses\n",
        "            }, 'best_model_allinone.pt')\n",
        "            print(\"‚òÖ Best model saved!\")\n",
        "\n",
        "    print(\"\\n‚úÖ Training Complete!\")\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\nüìä Final Evaluation\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Load best model for evaluation\n",
        "    checkpoint = torch.load('best_model_allinone.pt', map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Calculate perplexity\n",
        "    perplexity = calculate_perplexity(model, test_loader, device)\n",
        "    print(f\"Test Perplexity: {perplexity:.2f}\")\n",
        "\n",
        "    # Test generation\n",
        "    print(\"\\nüé≠ Test Generation Examples\")\n",
        "    print(\"-\" * 35)\n",
        "    test_inputs = [\"€å€Å ÿß€å⁄©\", \"Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫\", \"ÿß⁄Ü⁄æÿß\"]\n",
        "\n",
        "    for input_text in test_inputs:\n",
        "        generated = generate_text(model, tokenizer, train_dataset, input_text, max_length=20, device=device)\n",
        "        print(f\"Input:  {input_text}\")\n",
        "        print(f\"Output: {generated}\")\n",
        "        print()\n",
        "\n",
        "def chat_mode():\n",
        "    \"\"\"Interactive chat mode\"\"\"\n",
        "    print(\"üí¨ Urdu Chatbot - Interactive Mode\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"Type 'quit' to exit\")\n",
        "    print()\n",
        "\n",
        "    # Load model\n",
        "    try:\n",
        "        checkpoint = torch.load('best_model_allinone.pt', map_location=device)\n",
        "        config = checkpoint['config']\n",
        "        tokenizer = checkpoint['tokenizer']\n",
        "\n",
        "        # Create model\n",
        "        model = Transformer(\n",
        "            vocab_size=checkpoint['vocab_size'],\n",
        "            d_model=config['d_model'],\n",
        "            num_heads=config['num_heads'],\n",
        "            d_ff=config['d_ff'],\n",
        "            num_encoder_layers=config['num_encoder_layers'],\n",
        "            num_decoder_layers=config['num_decoder_layers'],\n",
        "            max_len=config['max_len'],\n",
        "            dropout=config['dropout'],\n",
        "            pad_idx=0\n",
        "        ).to(device)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.eval()\n",
        "\n",
        "        # Create dataset for special tokens\n",
        "        dataset = UrduChatbotDataset([], tokenizer, config['max_len'])\n",
        "\n",
        "        print(\"Model loaded successfully!\")\n",
        "        print()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå Model file not found! Please train the model first.\")\n",
        "        return\n",
        "\n",
        "    # Chat loop\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"You: \").strip()\n",
        "            if user_input.lower() == 'quit':\n",
        "                print(\"Goodbye! üëã\")\n",
        "                break\n",
        "\n",
        "            if not user_input:\n",
        "                continue\n",
        "\n",
        "            # Generate response\n",
        "            response = generate_text(model, tokenizer, dataset, user_input, max_length=30, device=device)\n",
        "            print(f\"Bot: {response}\")\n",
        "            print()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nGoodbye! üëã\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "def evaluate_model():\n",
        "    \"\"\"Evaluate the trained model\"\"\"\n",
        "    print(\"üìä Model Evaluation\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    try:\n",
        "        checkpoint = torch.load('best_model_allinone.pt', map_location=device)\n",
        "        config = checkpoint['config']\n",
        "        tokenizer = checkpoint['tokenizer']\n",
        "\n",
        "        # Load test data\n",
        "        sentence_groups = load_data()\n",
        "        normalized_groups = [normalize_urdu_text(group) for group in sentence_groups]\n",
        "\n",
        "        # Use last 10% as test set\n",
        "        test_size = int(0.1 * len(normalized_groups))\n",
        "        test_groups = normalized_groups[-test_size:]\n",
        "\n",
        "        # Create test dataset\n",
        "        test_dataset = UrduChatbotDataset(test_groups, tokenizer, config['max_len'])\n",
        "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        # Create model\n",
        "        model = Transformer(\n",
        "            vocab_size=checkpoint['vocab_size'],\n",
        "            d_model=config['d_model'],\n",
        "            num_heads=config['num_heads'],\n",
        "            d_ff=config['d_ff'],\n",
        "            num_encoder_layers=config['num_encoder_layers'],\n",
        "            num_decoder_layers=config['num_decoder_layers'],\n",
        "            max_len=config['max_len'],\n",
        "            dropout=config['dropout'],\n",
        "            pad_idx=0\n",
        "        ).to(device)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        # Calculate metrics\n",
        "        print(\"Calculating metrics...\")\n",
        "        perplexity = calculate_perplexity(model, test_loader, device)\n",
        "\n",
        "        print(f\"üìà Evaluation Results:\")\n",
        "        print(f\"Perplexity: {perplexity:.2f}\")\n",
        "\n",
        "        # Proper evaluation with test data pairs\n",
        "        print(\"\\nüîç Detailed Evaluation Metrics:\")\n",
        "        print(\"-\" * 35)\n",
        "\n",
        "        # Evaluate on actual test data pairs\n",
        "        bleu_scores = []\n",
        "        rouge_scores = []\n",
        "        chrf_scores = []\n",
        "\n",
        "        print(f\"\\nüé≠ Evaluating on Test Data:\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Sample a subset of test data for evaluation\n",
        "        eval_samples = min(100, len(test_groups))  # Evaluate on 100 samples or all if less\n",
        "\n",
        "        for i in range(eval_samples):\n",
        "            # Get input and target from test data\n",
        "            group = test_groups[i]\n",
        "            words = group.split()\n",
        "\n",
        "            # Split same way as training (2/5th input, 3/5th target)\n",
        "            total_words = len(words)\n",
        "            split_point = max(1, total_words * 2 // 5)\n",
        "\n",
        "            input_text = ' '.join(words[:split_point])\n",
        "            target_text = ' '.join(words[split_point:])\n",
        "\n",
        "            # Generate response\n",
        "            generated = generate_text(model, tokenizer, test_dataset, input_text, max_length=20, device=device)\n",
        "\n",
        "            # Calculate metrics against actual target\n",
        "            bleu = calculate_bleu_score(generated, target_text)\n",
        "            rouge = calculate_rouge_l(generated, target_text)\n",
        "            chrf = calculate_chrf(generated, target_text)\n",
        "\n",
        "            bleu_scores.append(bleu)\n",
        "            rouge_scores.append(rouge)\n",
        "            chrf_scores.append(chrf)\n",
        "\n",
        "            # Show first few examples\n",
        "            if i < 5:\n",
        "                print(f\"Sample {i+1}:\")\n",
        "                print(f\"Input:     {input_text}\")\n",
        "                print(f\"Generated: {generated}\")\n",
        "                print(f\"Target:    {target_text}\")\n",
        "                print(f\"BLEU: {bleu:.3f} | ROUGE-L: {rouge:.3f} | chrF: {chrf:.3f}\")\n",
        "                print()\n",
        "\n",
        "        # Average metrics\n",
        "        avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "        avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
        "        avg_chrf = sum(chrf_scores) / len(chrf_scores)\n",
        "\n",
        "        print(f\"üìä Evaluation Results (on {eval_samples} samples):\")\n",
        "        print(f\"BLEU Score:    {avg_bleu:.3f}\")\n",
        "        print(f\"ROUGE-L Score: {avg_rouge:.3f}\")\n",
        "        print(f\"chrF Score:    {avg_chrf:.3f}\")\n",
        "        print(f\"Perplexity:    {perplexity:.2f}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå Model file not found! Please train the model first.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during evaluation: {e}\")"
      ],
      "metadata": {
        "id": "qOuXwo8J65HI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Script to process cleaned Urdu text and create groups of 5 sentences.\n",
        "Each group represents a sliding window of 5 consecutive sentences.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import os\n",
        "\n",
        "def read_cleaned_text(file_path):\n",
        "    \"\"\"Read the cleaned text file and return all lines.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            lines = file.readlines()\n",
        "        return [line.strip() for line in lines if line.strip()]\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {file_path} not found.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return []\n",
        "\n",
        "def split_into_sentences(text_lines):\n",
        "    \"\"\"\n",
        "    Split text lines into sentences using Urdu full stop (€î) as delimiter.\n",
        "    Keep the delimiters with the sentences.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    for line in text_lines:\n",
        "        # Split by Urdu full stop but keep the delimiter\n",
        "        parts = re.split(r'(€î)', line)\n",
        "        current_sentence = \"\"\n",
        "        for part in parts:\n",
        "            if part == '€î':\n",
        "                # Add the delimiter to the current sentence\n",
        "                current_sentence += part\n",
        "                if current_sentence.strip():\n",
        "                    sentences.append(current_sentence.strip())\n",
        "                    current_sentence = \"\"\n",
        "            else:\n",
        "                current_sentence += part\n",
        "\n",
        "        # Add any remaining text without delimiter\n",
        "        if current_sentence.strip():\n",
        "            sentences.append(current_sentence.strip())\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def create_sentence_groups(sentences, group_size=5):\n",
        "    \"\"\"\n",
        "    Create groups of sentences using sliding window approach.\n",
        "    Each group contains 'group_size' consecutive sentences.\n",
        "    \"\"\"\n",
        "    groups = []\n",
        "    for i in range(len(sentences) - group_size + 1):\n",
        "        group = sentences[i:i + group_size]\n",
        "        groups.append(group)\n",
        "    return groups\n",
        "\n",
        "def save_groups_to_file(groups, output_file):\n",
        "    \"\"\"Save sentence groups to a file, one group per line.\"\"\"\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as file:\n",
        "            for group in groups:\n",
        "                # Join sentences in the group with a space\n",
        "                line = ' '.join(group)\n",
        "                file.write(line + '\\n')\n",
        "        print(f\"Successfully saved {len(groups)} groups to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving file: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to process the cleaned text and create sentence groups.\"\"\"\n",
        "    input_file = \"cleaned_urdu_text.txt\"\n",
        "    output_file = \"sentence_groups.txt\"\n",
        "\n",
        "    print(\"Reading cleaned text...\")\n",
        "    text_lines = read_cleaned_text(input_file)\n",
        "\n",
        "    if not text_lines:\n",
        "        print(\"No text found to process.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(text_lines)} lines of text.\")\n",
        "\n",
        "    print(\"Splitting into sentences...\")\n",
        "    sentences = split_into_sentences(text_lines)\n",
        "    print(f\"Found {len(sentences)} sentences.\")\n",
        "\n",
        "    print(\"Creating groups of 5 sentences...\")\n",
        "    groups = create_sentence_groups(sentences, group_size=5)\n",
        "    print(f\"Created {len(groups)} groups.\")\n",
        "\n",
        "    print(\"Saving groups to file...\")\n",
        "    save_groups_to_file(groups, output_file)\n",
        "\n",
        "    # Show first few examples\n",
        "    print(\"\\nFirst 3 groups:\")\n",
        "    for i, group in enumerate(groups[:3]):\n",
        "        print(f\"Group {i+1}: {' '.join(group)}\")\n",
        "\n",
        "    print(f\"\\nProcessing complete! Check {output_file} for results.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAZDVukjijbI",
        "outputId": "38fd1f9c-6cdc-41d7-cca9-e97f6ab08e05"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading cleaned text...\n",
            "Found 20044 lines of text.\n",
            "Splitting into sentences...\n",
            "Found 20145 sentences.\n",
            "Creating groups of 5 sentences...\n",
            "Created 20141 groups.\n",
            "Saving groups to file...\n",
            "Successfully saved 20141 groups to sentence_groups.txt\n",
            "\n",
            "First 3 groups:\n",
            "Group 1: ⁄©ÿ®⁄æ€å ⁄©ÿ®⁄æÿßÿ± €Å€å ÿÆ€åÿßŸÑ€å ŸæŸÑÿßŸà ÿ®ŸÜÿßÿ™ÿß €ÅŸà⁄∫ ÿßŸàÿ± Ÿæ⁄æÿ± ŸÖŸÖ⁄©ŸÜ €Å€í ⁄©€Å Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ÿ®⁄æ€å €ÅŸà €å€Å ŸÅ€åÿµŸÑ€Å ÿ®⁄æ€å ⁄Øÿ≤ÿ¥ÿ™€Å ÿØŸà ÿ≥ÿßŸÑ ŸÖ€å⁄∫ ÿßŸÜ ⁄©€í ÿ®ŸÑ€í ÿ®ÿßÿ≤Ÿà⁄∫ ⁄©€í ÿ≥ÿßŸÖŸÜ€í €ÅŸà ⁄Øÿß ÿ¢ÿ®€å ÿ¨ÿßŸÜŸàÿ± ŸÖ€å⁄∫ ÿ®ÿ∑ÿÆ ÿ®⁄ØŸÑÿß ÿßŸàÿ± ÿØŸàÿ≥ÿ±ÿß ÿ¢ÿ®€å Ÿæÿ±ŸÜÿØ€Å ÿ¥ÿßŸÖŸÑ €ÅŸàŸÜÿß\n",
            "Group 2: ÿßŸàÿ± Ÿæ⁄æÿ± ŸÖŸÖ⁄©ŸÜ €Å€í ⁄©€Å Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ÿ®⁄æ€å €ÅŸà €å€Å ŸÅ€åÿµŸÑ€Å ÿ®⁄æ€å ⁄Øÿ≤ÿ¥ÿ™€Å ÿØŸà ÿ≥ÿßŸÑ ŸÖ€å⁄∫ ÿßŸÜ ⁄©€í ÿ®ŸÑ€í ÿ®ÿßÿ≤Ÿà⁄∫ ⁄©€í ÿ≥ÿßŸÖŸÜ€í €ÅŸà ⁄Øÿß ÿ¢ÿ®€å ÿ¨ÿßŸÜŸàÿ± ŸÖ€å⁄∫ ÿ®ÿ∑ÿÆ ÿ®⁄ØŸÑÿß ÿßŸàÿ± ÿØŸàÿ≥ÿ±ÿß ÿ¢ÿ®€å Ÿæÿ±ŸÜÿØ€Å ÿ¥ÿßŸÖŸÑ €ÅŸàŸÜÿß ÿßÿ≥ ÿ∑€íÿ¥ÿØ€Å ŸÖÿØÿ™ ŸÖ€å⁄∫ ŸÖ€åÿ±€í ÿ∞ŸÖ€í Ÿæ⁄ë⁄æÿßŸÜ€í ⁄©ÿß ⁄©ÿßŸÖ ⁄©ÿßŸÅ€å €ÅŸÑ⁄©ÿß €Å€í\n",
            "Group 3: €å€Å ŸÅ€åÿµŸÑ€Å ÿ®⁄æ€å ⁄Øÿ≤ÿ¥ÿ™€Å ÿØŸà ÿ≥ÿßŸÑ ŸÖ€å⁄∫ ÿßŸÜ ⁄©€í ÿ®ŸÑ€í ÿ®ÿßÿ≤Ÿà⁄∫ ⁄©€í ÿ≥ÿßŸÖŸÜ€í €ÅŸà ⁄Øÿß ÿ¢ÿ®€å ÿ¨ÿßŸÜŸàÿ± ŸÖ€å⁄∫ ÿ®ÿ∑ÿÆ ÿ®⁄ØŸÑÿß ÿßŸàÿ± ÿØŸàÿ≥ÿ±ÿß ÿ¢ÿ®€å Ÿæÿ±ŸÜÿØ€Å ÿ¥ÿßŸÖŸÑ €ÅŸàŸÜÿß ÿßÿ≥ ÿ∑€íÿ¥ÿØ€Å ŸÖÿØÿ™ ŸÖ€å⁄∫ ŸÖ€åÿ±€í ÿ∞ŸÖ€í Ÿæ⁄ë⁄æÿßŸÜ€í ⁄©ÿß ⁄©ÿßŸÖ ⁄©ÿßŸÅ€å €ÅŸÑ⁄©ÿß €Å€í ŸÅŸÑŸÖ€å⁄∫ ÿØ€å⁄©⁄æŸÜ€å €ÅŸà⁄∫€î\n",
            "\n",
            "Processing complete! Check sentence_groups.txt for results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. MAIN EXECUTION\n"
      ],
      "metadata": {
        "id": "iRDVn7sC7CJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(mode, resume=False):\n",
        "    \"\"\"\n",
        "    Main function for Urdu Conversational Chatbot.\n",
        "\n",
        "    Args:\n",
        "        mode (str): One of 'train', 'chat', or 'eval'.\n",
        "        resume (bool): Whether to resume training from checkpoint (only for 'train' mode).\n",
        "    \"\"\"\n",
        "    if mode == 'train':\n",
        "        train_model(resume=resume)\n",
        "    elif mode == 'chat':\n",
        "        chat_mode()\n",
        "    elif mode == 'eval':\n",
        "        evaluate_model()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid mode. Choose 'train', 'chat', or 'eval'.\")\n",
        "\n",
        "\n",
        "mode = input(\"Enter mode (train/chat/eval): \").strip().lower()\n",
        "\n",
        "resume = False\n",
        "if mode == \"train\":\n",
        "    ans = input(\"Resume training from last checkpoint? (y/n): \").strip().lower()\n",
        "    resume = ans == \"y\"\n",
        "\n",
        "main(mode, resume)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "-OhqkjgP7Dpy",
        "outputId": "c8490798-ea02-4feb-f0b3-1ba1318fcac2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter mode (train/chat/eval): chat\n",
            "üí¨ Urdu Chatbot - Interactive Mode\n",
            "========================================\n",
            "Type 'quit' to exit\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.BPETokenizer was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.BPETokenizer])` or the `torch.serialization.safe_globals([__main__.BPETokenizer])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2092158618.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mresume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mans\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2092158618.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(mode, resume)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'chat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mchat_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2997529557.py\u001b[0m in \u001b[0;36mchat_mode\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m# Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model_allinone.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1527\u001b[0m                         )\n\u001b[1;32m   1528\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m                 return _load(\n\u001b[1;32m   1531\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.BPETokenizer was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.BPETokenizer])` or the `torch.serialization.safe_globals([__main__.BPETokenizer])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ]
    }
  ]
}